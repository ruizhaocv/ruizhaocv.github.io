{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

# Selected Publications

Check out full publication list at my Google Scholar profile: 
<a href='https://scholar.google.com/citations?user=wYs7vogAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>.

[//]: # (Check out full publication list at my <a href='https://scholar.google.com/citations?user=wYs7vogAAAAJ'>Google Scholar profile</a>.)


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><a href="images/EvolveDirector.png"><img src='images/DoraCycle_teaser.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles</b><br>
<i>The IEEE Conference on Computer Vision and Pattern Recognition, 2025</i><br>
<b>Rui Zhao</b>, Weijia Mao, Mike Zheng Shou.<br>
[<a href="https://arxiv.org/abs/2503.03651">arXiv</a>]
[[Github ![](https://img.shields.io/github/stars/showlab/DoraCycle?style=social)](https://github.com/showlab/DoraCycle)]<br>
<div style="text-align: justify">
  
</div>
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><a href="images/EvolveDirector.png"><img src='images/EvolveDirector.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models</b><br>
<i>The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024</i><br>
<b>Rui Zhao</b>, Hangjie Yuan, Yujie Wei, Shiwei Zhang, Yuchao Gu, Lingmin Ran, Xiang Wang, Jay Wu, David Zhang, Yingya Zhang, Mike Zheng Shou.<br>
[<a href="https://arxiv.org/abs/2410.07133">arXiv</a>][<a href="https://huggingface.co/ruizhaocv/Edgen">Hugging Face</a>]
[[Github ![](https://img.shields.io/github/stars/showlab/EvolveDirector?style=social)](https://github.com/showlab/EvolveDirector)]<br>
<div style="text-align: justify">
  
</div>
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><a href="images/MotionDirector_teaser.gif"><img src='images/MotionDirector_teaser.gif' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>MotionDirector: Motion Customization of Text-to-Video Diffusion Models</b><br>
<i>Proceedings of the European Conference on Computer Vision, 2024</i><br>
<b>Rui Zhao</b>, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jiawei Liu, Weijia Wu, Jussi Keppo, Mike Zheng Shou<br>
[<a href="https://showlab.github.io/MotionDirector/">Project Page</a>][<a href="https://arxiv.org/abs/2310.08465">arXiv</a>]
[[Github ![](https://img.shields.io/github/stars/showlab/MotionDirector?style=social)](https://github.com/showlab/MotionDirector)]<br>
🎙️ <b>Oral</b> Presentation, Acceptance Rate: 2.3%. <br>
🤗 Featured in Hugging Face "[Spaces of the Week 🔥](https://huggingface.co/spaces)" trending list. <br>
📃 Featured in "Top 40 most cited papers of ECCV 2024" [list](https://mp.weixin.qq.com/s/5FyBKVH4rc608MC84_-RdA) by AIR-SUN.
<div style="text-align: justify">
  
</div>
</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2023</div><a href="images/T2P_teaser.png"><img src='images/T2P_teaser.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>Zero-Shot Text-to-Parameter Translation for Game Character Auto-Creation</b><br>
<i>The IEEE Conference on Computer Vision and Pattern Recognition, 2023</i><br>
<b>Rui Zhao</b>, Wei Li, Zhipeng Hu, Lincheng Li, Zhengxia Zou, Zhenwei Shi, and Changjie Fan<br>
[<a href="https://arxiv.org/abs/2303.01311">PDF</a>]<br>
<div style="text-align: justify">
<b>Featured apps: </b> <a href="https://h.163.com/">Justice Mobile</a> (The first plausible solution for text-driven game characters auto-creation.).<br>

<b>Media coverage: </b><a href="https://www.newscientist.com/article/2362730-character-creator-ai-puts-barack-obama-or-anyone-in-a-video-game/">[NewScientist] Character creator AI puts Barack Obama – or anyone – in a video game</a> || 
<a href="https://news.yxrb.net/2023/0314/1372.html">[游戏日报] 网易伏羲“文字捏脸”方向论文入选CVPR会议，称已应用于《逆水寒手游》</a> || 
<a href="https://mp.weixin.qq.com/s/I4MVjk7rIZIl0g8OY6xobg">CVPR 2023 | 网易伏羲5篇论文入选 包含文字捏脸等业内首创工作</a> ||
<a href="https://automaton-media.com/articles/newsjp/20230314-240584/">[AUTOMATON] Researchers report technological breakthroughs in using AI to “make characters by asking with words”.</a> (in Japanese, for English version, please refer to <a href="https://www.newsdirectory3.com/researchers-report-technological-breakthroughs-in-using-ai-to-make-characters-by-asking-with-words-a-wide-range-of-text-can-be-identified-from-animals-to-celebrities-automaton/">reprint 1</a> and <a href="https://www.archyde.com/researchers-report-on-the-technological-achievements-of-using-ai-to-make-characters-just-by-asking-with-words-a-wide-range-of-text-can-be-specified-from-animals-to-celebrities-automaton/">reprint 2</a>) || 
<a href="https://nazology.net/archives/123271">[Nazology] "Character creation support AI" that creates the ideal face specified in the sentence is now available!</a> .<br>


